{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw5Psr_1F0rW",
        "outputId": "a6c6e5b5-5f92-4979-ca85-60abef336df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated image IDs in 'a1.json' by reducing them by 43260.\n",
            "Updated image IDs in 'a2.json' by reducing them by 14398.\n",
            "Loaded data from cv_data1_adjusted.json\n",
            "ID: 41, Label: Trucks\n",
            "ID: 42, Label: No Trucks\n",
            "ID: 43, Label: No Trucks\n",
            "ID: 44, Label: No Trucks\n",
            "ID: 45, Label: Trucks\n",
            "ID: 46, Label: Trucks\n",
            "ID: 47, Label: No Trucks\n",
            "ID: 48, Label: Trucks\n",
            "ID: 49, Label: No Trucks\n",
            "ID: 50, Label: Trucks\n",
            "ID: 51, Label: No Trucks\n",
            "ID: 52, Label: Trucks\n",
            "ID: 53, Label: Trucks\n",
            "ID: 54, Label: No Trucks\n",
            "ID: 55, Label: Trucks\n",
            "ID: 56, Label: Trucks\n",
            "ID: 57, Label: Trucks\n",
            "ID: 58, Label: Trucks\n",
            "ID: 59, Label: No Trucks\n",
            "ID: 60, Label: Trucks\n",
            "Loaded data from cv_data2_adjusted.json\n",
            "ID: 41, Label: Trucks\n",
            "ID: 42, Label: No Trucks\n",
            "ID: 43, Label: No Trucks\n",
            "ID: 44, Label: No Trucks\n",
            "ID: 45, Label: No Trucks\n",
            "ID: 46, Label: Trucks\n",
            "ID: 47, Label: No Trucks\n",
            "ID: 48, Label: No Trucks\n",
            "ID: 49, Label: No Trucks\n",
            "ID: 50, Label: Trucks\n",
            "ID: 51, Label: No Trucks\n",
            "ID: 52, Label: Trucks\n",
            "ID: 53, Label: No Trucks\n",
            "ID: 54, Label: No Trucks\n",
            "ID: 55, Label: Trucks\n",
            "ID: 56, Label: Trucks\n",
            "ID: 57, Label: Trucks\n",
            "ID: 58, Label: Trucks\n",
            "ID: 59, Label: No Trucks\n",
            "ID: 60, Label: Trucks\n",
            "Loaded data from /content/CV data3.json\n",
            "ID: 41, Label: Trucks\n",
            "ID: 42, Label: No Trucks\n",
            "ID: 43, Label: No Trucks\n",
            "ID: 44, Label: No Trucks\n",
            "ID: 45, Label: No Trucks\n",
            "ID: 46, Label: Trucks\n",
            "ID: 47, Label: No Trucks\n",
            "ID: 48, Label: Trucks\n",
            "ID: 49, Label: No Trucks\n",
            "ID: 50, Label: Trucks\n",
            "ID: 51, Label: No Trucks\n",
            "ID: 52, Label: Trucks\n",
            "ID: 53, Label: No Trucks\n",
            "ID: 54, Label: No Trucks\n",
            "ID: 55, Label: Trucks\n",
            "ID: 56, Label: Trucks\n",
            "ID: 57, Label: Trucks\n",
            "ID: 58, Label: Trucks\n",
            "ID: 59, Label: No Trucks\n",
            "ID: 60, Label: Trucks\n",
            "Fleiss' Kappa Matrix:\n",
            "[[3 0]\n",
            " [0 3]\n",
            " [0 3]\n",
            " [0 3]\n",
            " [1 2]\n",
            " [3 0]\n",
            " [0 3]\n",
            " [2 1]\n",
            " [0 3]\n",
            " [3 0]\n",
            " [0 3]\n",
            " [3 0]\n",
            " [1 2]\n",
            " [0 3]\n",
            " [3 0]\n",
            " [3 0]\n",
            " [3 0]\n",
            " [3 0]\n",
            " [0 3]\n",
            " [3 0]]\n",
            "Fleiss' Kappa: 0.7998\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "# Load json\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Adjust image IDs by a specified amount because of id not matching error\n",
        "def adjust_image_ids(data, adjust_value):\n",
        "    for entry in data:\n",
        "        entry['id'] -= adjust_value  # Adjust the image ID by the specified value\n",
        "    return data\n",
        "\n",
        "# Save the modified JSON data\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "# Sync annotations from multiple annotators\n",
        "def sync_annotations(annotator_files):\n",
        "    \"\"\"\n",
        "    Sync annotations by image ID across various annotators.\n",
        "    \"\"\"\n",
        "    synchronized_data = {}\n",
        "    for file_path in annotator_files:\n",
        "        data = load_json(file_path)\n",
        "        print(f\"Loaded data from {file_path}\")\n",
        "\n",
        "        for entry in data:\n",
        "            img_id = entry['id']\n",
        "            annotations = entry.get('annotations', [])\n",
        "            if annotations and 'result' in annotations[0]:\n",
        "                result = annotations[0]['result']\n",
        "                if result and 'value' in result[0] and 'choices' in result[0]['value']:\n",
        "                    label = result[0]['value']['choices'][0]\n",
        "                    synchronized_data.setdefault(img_id, []).append(label)\n",
        "                    print(f\"ID: {img_id}, Label: {label}\")\n",
        "\n",
        "    return synchronized_data\n",
        "\n",
        "# Ensure consistent annotations from all annotators\n",
        "def filter_consistent_annotations(synchronized_data, annotator_count):\n",
        "\n",
        "    return {img_id: annotations for img_id, annotations in synchronized_data.items() if len(annotations) == annotator_count}\n",
        "\n",
        "# Prepare the Fleiss' Kappa matrix\n",
        "def prepare_fleiss_kappa_matrix(synchronized_data, categories):\n",
        "    \"\"\"\n",
        "    Create the Fleiss' Kappa matrix from synchronized annotations.\n",
        "    \"\"\"\n",
        "    matrix = np.zeros((len(synchronized_data), len(categories)), dtype=int)\n",
        "    for i, (img_id, annotations) in enumerate(synchronized_data.items()):\n",
        "        for label in annotations:\n",
        "            if label in categories:\n",
        "                matrix[i, categories.index(label)] += 1\n",
        "    return matrix\n",
        "\n",
        "\n",
        "# Annotator files\n",
        "annotator_files = [\n",
        "        '/content/CV data1.json',\n",
        "        '/content/CV data2.json',\n",
        "        '/content/CV data3.json'\n",
        "    ]\n",
        "\n",
        "# Adjust image IDs for the first two files and save\n",
        "adjust_value_a1 = 43260\n",
        "adjust_value_a2 = 14398\n",
        "\n",
        "# Adjust image IDs for 'a1' and 'a2' files and save them\n",
        "data_a1 = adjust_image_ids(load_json(annotator_files[0]), adjust_value_a1)\n",
        "save_json(data_a1, 'cv_data1_adjusted.json')\n",
        "print(f\"Updated image IDs in 'a1.json' by reducing them by {adjust_value_a1}.\")\n",
        "\n",
        "data_a2 = adjust_image_ids(load_json(annotator_files[1]), adjust_value_a2)\n",
        "save_json(data_a2, 'cv_data2_adjusted.json')\n",
        "print(f\"Updated image IDs in 'a2.json' by reducing them by {adjust_value_a2}.\")\n",
        "\n",
        "# Load and sync annotations from all annotator files\n",
        "synced_data = sync_annotations(['cv_data1_adjusted.json', 'cv_data2_adjusted.json', annotator_files[2]])\n",
        "\n",
        "# Filter consistent annotations (all annotators must annotate the same image)\n",
        "annotator_count = len(annotator_files)\n",
        "synced_data = filter_consistent_annotations(synced_data, annotator_count)\n",
        "\n",
        "# If no consistent data, print a warning and exit\n",
        "if not synced_data:\n",
        "    print(\"No consistent data found (some images don't have annotations from all annotators).\")\n",
        "else:\n",
        "    categories = [\"Trucks\", \"No Trucks\"]\n",
        "    matrix = prepare_fleiss_kappa_matrix(synced_data, categories)\n",
        "    print(\"Fleiss' Kappa Matrix:\")\n",
        "    print(matrix)\n",
        "\n",
        "    # Calculate Fleiss' Kappa\n",
        "    if matrix.shape[1] < 2 or np.all(np.sum(matrix, axis=1) <= 1):\n",
        "        print(\"Insufficient data: Fleiss' Kappa requires multiple annotations per image.\")\n",
        "    else:\n",
        "        kappa = fleiss_kappa(matrix)\n",
        "        print(f\"Fleiss' Kappa: {kappa:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fleiss kappa score of 0.7998 indicate that the annotators have labelled the data almost in the same way but whereas there are some differences which might be due to some misunderstandings of the data such as there were trucks hidden in the background which would normally be not clearly visible to some annotators which has led to that fleiss kappa score."
      ],
      "metadata": {
        "id": "qf-AE67xpwZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load json\n",
        "file_path_annotator_1 = \"/content/updated_nlp_data1.json\"\n",
        "file_path_annotator_2 = \"/content/updated_nlp_data2.json\"\n",
        "\n",
        "with open(file_path_annotator_1, \"r\") as file1, open(file_path_annotator_2, \"r\") as file2:\n",
        "    data_annotator_1 = json.load(file1)\n",
        "    data_annotator_2 = json.load(file2)\n",
        "\n",
        "annotator_1_labels = []\n",
        "annotator_2_labels = []\n",
        "\n",
        "#function to extract labels\n",
        "def extract_label(task):\n",
        "    try:\n",
        "        return task[\"annotations\"][0][\"result\"][0][\"value\"][\"labels\"][0]\n",
        "    except (IndexError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Create mappings for task IDs to labels\n",
        "annotations_1 = {task[\"id\"]: extract_label(task) for task in data_annotator_1}\n",
        "annotations_2 = {task[\"id\"]: extract_label(task) for task in data_annotator_2}\n",
        "\n",
        "# Match tasks and collect labels for common task IDs\n",
        "common_task_ids = set(annotations_1.keys()) & set(annotations_2.keys())\n",
        "for task_id in common_task_ids:\n",
        "    label_1 = annotations_1[task_id]\n",
        "    label_2 = annotations_2[task_id]\n",
        "    if label_1 is not None and label_2 is not None:\n",
        "        annotator_1_labels.append(label_1)\n",
        "        annotator_2_labels.append(label_2)\n",
        "\n",
        "# Calculate Cohen's kappa\n",
        "if annotator_1_labels and annotator_2_labels:\n",
        "    kappa = cohen_kappa_score(annotator_1_labels, annotator_2_labels)\n",
        "    print(f\"Cohen's kappa: {kappa}\")\n",
        "else:\n",
        "    print(\"No matching tasks with valid labels found or not enough data for calculation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtjDQeTrSjBq",
        "outputId": "84a7be5c-4b6b-4a22-b756-40c09773f8dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's kappa: 0.7297297297297298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohen's kappa score of 0.7297 indicates that both the annotators are thinking in the same way about the POS tags but there may be some misunderstandings such as should the word be considered as PROPN or NOUN and so on which led to that cohen's kappa score."
      ],
      "metadata": {
        "id": "bS-FPmDiqtFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this code is used for matching the ids in the json file\n",
        "\n",
        "import json\n",
        "\n",
        "# Load json\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Save the updated json\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "# Adjust image IDs\n",
        "def adjust_image_ids(data, adjusted_val):\n",
        "    for item in data:\n",
        "        item['id'] -= adjusted_val\n",
        "    return data\n",
        "\n",
        "file_path_1 = '/content/NLP data1.json'\n",
        "file_path_2 = '/content/NLP data2.json'\n",
        "\n",
        "# Load data\n",
        "data1 = load_json(file_path_1)\n",
        "data2 = load_json(file_path_2)\n",
        "\n",
        "min_id_1 = min(item['id'] for item in data1)\n",
        "min_id_2 = min(item['id'] for item in data2)\n",
        "\n",
        "# Choose the smaller of the two minimums\n",
        "adjusted_val = min(min_id_1, min_id_2)\n",
        "\n",
        "# Reduce IDs\n",
        "updated_data1 = adjust_image_ids(data1, 14957)\n",
        "updated_data2 = adjust_image_ids(data2, 0)\n",
        "\n",
        "# Save the updated data back into separate JSON files\n",
        "save_json(updated_data1, 'updated_nlp_data1.json')\n",
        "save_json(updated_data2, 'updated_nlp_data2.json')\n",
        "\n",
        "print(f\"IDs in both files have been reduced by {reduction_value}.\")\n",
        "print(\"Updated annotations saved to 'updated_nlp_data1.json' and 'updated_nlp_data2.json'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyop8lVwHD8f",
        "outputId": "27aa4ca3-14f1-4286-b0c0-85ccbdb7306a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDs in both files have been reduced by 14460.\n",
            "Updated annotations saved to 'updated_nlp_data1.json' and 'updated_nlp_data2.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code was used for updating the ids in the annotations present in the json file because there are errors occuring due to mismatching of ids in the json files"
      ],
      "metadata": {
        "id": "FnrfPtlKthXD"
      }
    }
  ]
}